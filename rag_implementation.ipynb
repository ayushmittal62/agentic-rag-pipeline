{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOy3LMBx7zRUNu5/nzlbtcw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayushmittal62/agentic-rag-pipeline/blob/main/rag_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgImARkWn22i"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if \"COLAB_GPU\" in os.environ:\n",
        "    print(\"[INFO] Running in Google Colab, installing requirements.\")\n",
        "    !pip install -U torch # requires torch 2.1.1+ (for efficient sdpa\n",
        "    !pip install PyMuPDF # for reading PDFs with Python\n",
        "    !pip install tqdm # for progress bars\n",
        "    !pip install sentence-transformers # for embedding models\n",
        "    !pip install accelerate # for quantization model loading\n",
        "    !pip install bitsandbytes # for quantizing models (less storage space)\n",
        "    !pip install flash-attn --no-build-isolation # for faster attention"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "pdf_path = \"Hands_on_ml_new_file.pdf\"\n",
        "url = \"enter the url\"\n",
        "'''https://www.clc.hcmus.edu.vn/wp-content/uploads/2017/11/\n",
        "Hands_On_Machine_Learning_with_Scikit_Learn_and_TensorFlow.pdf\n",
        "'''\n",
        "if not os.path.exists(pdf_path):\n",
        "    print(\"File doesn't exist.. Downloading PDF...\")\n",
        "\n",
        "\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "                      \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                      \"Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        with open(pdf_path, \"wb\") as file:\n",
        "            file.write(response.content)\n",
        "        print(f\"The file has been downloaded and saved as {pdf_path}\")\n",
        "    else:\n",
        "        print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
        "else:\n",
        "    print(f\"File {pdf_path} exists.\")\n"
      ],
      "metadata": {
        "id": "NpKzrDx_oa9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def text_formatter(text: str) -> str:\n",
        "    return text.replace(\"\\n\", \" \").strip()\n",
        "\n",
        "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
        "    doc = fitz.open(pdf_path)\n",
        "    pages_and_texts = []\n",
        "\n",
        "    for page_number, page in tqdm(enumerate(doc)):\n",
        "        page_text = page.get_text()\n",
        "        text = text_formatter(page_text)\n",
        "\n",
        "        pages_and_texts.append({\n",
        "            \"page_number\": page_number,  # or page_number - 15 if needed\n",
        "            \"page_char_count\": len(text),\n",
        "            \"page_word_count\": len(text.split(\" \")),\n",
        "            \"page_sentence_count_raw\": len(text.split(\". \")),\n",
        "            \"page_token_count\": len(text) / 4,\n",
        "            \"text\": text\n",
        "        })\n",
        "\n",
        "    return pages_and_texts   # now outside the loop\n",
        "\n",
        "pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)\n",
        "pages_and_texts[20]\n"
      ],
      "metadata": {
        "id": "Myf75BocqyOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.sample(pages_and_texts, k=2)"
      ],
      "metadata": {
        "id": "Hn8_msRytP7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(pages_and_texts)\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "o-qtXVzNtfyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe().round(2)"
      ],
      "metadata": {
        "id": "x7jNOWR3t92r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.lang.en import English\n",
        "\n",
        "nlp = English()\n",
        "nlp.add_pipe(\"sentencizer\")\n",
        "\n",
        "doc = nlp(\"This is a sentence. This is another sentence.\")\n",
        "assert len(list(doc.sents)) == 2\n",
        "list(doc.sents)"
      ],
      "metadata": {
        "id": "vGHWD237ur9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for item in tqdm(pages_and_texts):\n",
        "    item[\"sentences\"] = list(nlp(item[\"text\"]).sents)\n",
        "    item[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]]\n",
        "    item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])"
      ],
      "metadata": {
        "id": "wr-_yPnjv1It"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.sample(pages_and_texts, k=1)"
      ],
      "metadata": {
        "id": "pkRpCaukwtui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(pages_and_texts)\n",
        "df.describe().round(2)"
      ],
      "metadata": {
        "id": "PzlKNapQw3l2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_sent_chunk_size = 13\n",
        "def split_list(input_list: list,\n",
        "               slice_size: int) -> list[list]:\n",
        "    return [input_list[i:i + slice_size] for i in range(\n",
        "        0, len(input_list), slice_size)]\n",
        "\n",
        "for item in tqdm(pages_and_texts):\n",
        "    item[\"sentences_chunks\"] = split_list(\n",
        "        item[\"sentences\"],slice_size= num_sent_chunk_size)\n",
        "    item[\"num_sentences_chunks\"] = len(item[\"sentences_chunks\"])"
      ],
      "metadata": {
        "id": "UVcKL8lkxWYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.sample(pages_and_texts, k=1)"
      ],
      "metadata": {
        "id": "XtBb3IDvyHbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(pages_and_texts)\n",
        "df.describe().round(2)"
      ],
      "metadata": {
        "id": "Vpq8yOzUyUdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "pages_and_chunks = []\n",
        "for item in tqdm(pages_and_texts):\n",
        "  for sentence_chunk in item[\"sentences_chunks\"]:\n",
        "    chunk_dict = {}\n",
        "    chunk_dict[\"page_number\"] = item[\"page_number\"]\n",
        "\n",
        "    joined_sentence_chunk = \"\".join(sentence_chunk).replace(\"  \", \" \").strip()\n",
        "    joined_sentence_chunk = re.sub(\n",
        "        r'\\.([A-Z])', r'. \\1', joined_sentence_chunk)\n",
        "    chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n",
        "\n",
        "    chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
        "    chunk_dict[\"chunk_word_count\"] = len(\n",
        "        [word for word in joined_sentence_chunk.split(\" \")])\n",
        "    chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4\n",
        "\n",
        "    pages_and_chunks.append(chunk_dict)\n",
        "\n",
        "len(pages_and_chunks)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AhOSXrzX1qJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.sample(pages_and_chunks, k=1)"
      ],
      "metadata": {
        "id": "JA6GeAM03zMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(pages_and_chunks)\n",
        "df.describe().round(2)"
      ],
      "metadata": {
        "id": "kQpe6MQK4Ad3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_token_length = 30\n",
        "for row in df[df[\"chunk_token_count\"] <= min_token_length].sample(10).iterrows():\n",
        "    print(\n",
        "      f'Chunk token count: {row[1][\"chunk_token_count\"]} | Text: {row[1][\"sentence_chunk\"]}')\n"
      ],
      "metadata": {
        "id": "pifF_b294Kr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pages_and_chunks_over_min_token_len = df[\n",
        "    df[\"chunk_token_count\"] > min_token_length].to_dict(orient=\"records\")\n",
        "pages_and_chunks_over_min_token_len[:2]\n"
      ],
      "metadata": {
        "id": "J2SjlY6146bY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe().round(2)"
      ],
      "metadata": {
        "id": "q8nVKeBL5Ysh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding Part a head   "
      ],
      "metadata": {
        "id": "uS6batC76YEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "embedding_model = SentenceTransformer(model_name_or_path =\"all-mpnet-base-v2\",\n",
        "                                      device=\"cpu\")\n",
        "sentences = [\n",
        "    \"The Sentences Transformers library provides an easy and open-source way to create embeddings.\",\n",
        "    \"Sentences can be embedded one by one or as a list of strings.\",\n",
        "    \"Embeddings are one of the most powerful concepts in machine learning!\",\n",
        "    \"Learn to use embeddings well and you'll be well on your way to being an AI engineer.\"\n",
        "]\n",
        "\n",
        "embeddings = embedding_model.encode(sentences)\n",
        "embeddings_dict =  dict(zip(sentences, embeddings))\n",
        "\n",
        "for sentence, embedding in embeddings_dict.items():\n",
        "    print(f'Sentence: {sentence} | Embedding: {embedding}')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "l_PBtsxa5h6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "single_sentence = \"Yo! How cool are embeddings?\"\n",
        "single_embedding = embedding_model.encode(single_sentence)\n",
        "print(f\"Sentence: {single_sentence}\")\n",
        "print(f\"Embedding:\\n{single_embedding}\")\n",
        "print(f\"Embedding size: {single_embedding.shape}\")"
      ],
      "metadata": {
        "id": "CBRUdYFR7r-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Uncomment to see how long it takes to create embeddings on CPU\n",
        "# # Make sure the model is on the CPU\n",
        "embedding_model.to(\"cpu\")\n",
        "\n",
        "# Embed each chunk one by one\n",
        "for item in tqdm(pages_and_chunks_over_min_token_len):\n",
        "    item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"])\n"
      ],
      "metadata": {
        "id": "hJTPnAFg76Jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn text chunks into a single list\n",
        "text_chunks = [\n",
        "    item[\"sentence_chunk\"] for item in pages_and_chunks_over_min_token_len]\n"
      ],
      "metadata": {
        "id": "u7X6DBUHAnfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Embed all texts in batches\n",
        "text_chunk_embeddings = embedding_model.encode(\n",
        "    text_chunks,\n",
        "    batch_size=32,               # adjust based on your GPU/CPU memory\n",
        "    convert_to_tensor=True,      # returns torch.Tensor\n",
        "    show_progress_bar=True       # âœ… adds tqdm progress bar\n",
        ")\n",
        "\n",
        "text_chunk_embeddings\n"
      ],
      "metadata": {
        "id": "vFGHnUjxBC3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_chunks_and_embeddings_df = pd.DataFrame(pages_and_chunks_over_min_token_len)\n",
        "embeddings_df_save_path = \"text_chunks_and_embeddings_df.csv\"\n",
        "text_chunks_and_embeddings_df.to_csv(embeddings_df_save_path, index=False)"
      ],
      "metadata": {
        "id": "ZCtIfwdkHI3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_chunks_and_embedding_df_load = pd.read_csv(embeddings_df_save_path)\n",
        "text_chunks_and_embedding_df_load.sample(5)"
      ],
      "metadata": {
        "id": "2NdafgSHHWug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load your CSV file\n",
        "text_chunks_and_embedding_df = pd.read_csv(\"text_chunks_and_embeddings_df.csv\")\n",
        "\n",
        "# Convert the string embeddings \"[0.1 0.2 ...]\" into numpy arrays\n",
        "text_chunks_and_embedding_df['embedding'] = text_chunks_and_embedding_df['embedding'].apply(\n",
        "    lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \")\n",
        ")\n",
        "\n",
        "# Convert to list of dicts for convenience\n",
        "pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n",
        "\n",
        "# Convert embeddings to torch tensor\n",
        "embeddings = torch.tensor(\n",
        "    np.array(text_chunks_and_embedding_df[\"embedding\"].tolist()),\n",
        "    dtype=torch.float32\n",
        ").to(device)\n",
        "\n",
        "print(embeddings.shape)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tNx_7hFBK84f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_chunks_and_embedding_df.head()"
      ],
      "metadata": {
        "id": "AbNDWGwjZSzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings[0]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "oJbqPHfpZX2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import util, SentenceTransformer\n",
        "\n",
        "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\",\n",
        "                                      device=device)"
      ],
      "metadata": {
        "id": "gXnDuBiXZfyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Define the query\n",
        "query = \"What do you mean by Unsupervised Learning\"\n",
        "print(f\"Query: {query}\")\n",
        "\n",
        "# 2. Embed the query to the same numerical space as the text examples\n",
        "# Note: It's important to embed your query with the same model\n",
        "query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
        "\n",
        "# 3. Get similarity scores with the dot product (we'll time this for fun)\n",
        "from time import perf_counter as timer\n",
        "\n",
        "start_time = timer()\n",
        "dot_scores = util.dot_score(a=query_embedding, b=embeddings)[0]\n",
        "end_time = timer()\n",
        "\n",
        "print(\n",
        "    f\"Time take {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")\n",
        "\n",
        "# 4. Get the top-k results (we'll keep this to 5)\n",
        "top_results_dot_product = torch.topk(dot_scores, k=3)\n",
        "top_results_dot_product\n",
        "\n"
      ],
      "metadata": {
        "id": "w-4NPYMRZ3Bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "larger_embeddings = torch.randn(100*embeddings.shape[0], 768).to(device)\n",
        "print(f\"Embeddings shape: {larger_embeddings.shape}\")\n",
        "\n",
        "# Perform dot product across 168,000 embeddings\n",
        "start_time = timer()\n",
        "dot_scores = util.dot_score(a=query_embedding, b=larger_embeddings)[0]\n",
        "end_time = timer()\n",
        "\n",
        "print(\n",
        "    f\"Time take {len(larger_embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Na0qZbHIaXoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "def print_wrapped(text, wrap_length=80):\n",
        "    wrapped_text = textwrap.fill(text, wrap_length)\n",
        "    print(wrapped_text)\n"
      ],
      "metadata": {
        "id": "apf_RaeJaqfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Query: '{query}'\\n\")\n",
        "print(\"Results:\")\n",
        "# Loop through zipped together scores and indicies from torch.topk\n",
        "for score, idx in zip(top_results_dot_product[0], top_results_dot_product[1]):\n",
        "    print(f\"Score: {score:.4f}\")\n",
        "    # Print relevant sentence chunk (since the scores are in descending order,\n",
        "    print(\"Text:\")\n",
        "    print_wrapped(pages_and_chunks[idx][\"sentence_chunk\"])\n",
        "    # Print the page number too so we can reference the textbook further\n",
        "    print(f\"Page number: {pages_and_chunks[idx]['page_number']}\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "OqbIxymMaswR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def dot_product(vector1, vector2):\n",
        "    return torch.dot(vector1, vector2)\n",
        "\n",
        "def cosine_similarity(vector1, vector2):\n",
        "    dot_product = torch.dot(vector1, vector2)\n",
        "\n",
        "    # Get Euclidean/L2 norm of each vector (removes the magnitude\n",
        "    norm_vector1 = torch.sqrt(torch.sum(vector1**2))\n",
        "    norm_vector2 = torch.sqrt(torch.sum(vector2**2))\n",
        "\n",
        "    return dot_product / (norm_vector1 * norm_vector2)\n",
        "\n",
        "# Example tensors\n",
        "vector1 = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
        "vector2 = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
        "vector3 = torch.tensor([4, 5, 6], dtype=torch.float32)\n",
        "vector4 = torch.tensor([-1, -2, -3], dtype=torch.float32)\n",
        "\n",
        "# Calculate dot product\n",
        "print(\"Dot product between vector1 and vector2:\", dot_product(vector1, vector2))\n",
        "print(\"Dot product between vector1 and vector3:\", dot_product(vector1, vector3))\n",
        "print(\"Dot product between vector1 and vector4:\", dot_product(vector1, vector4))\n",
        "\n",
        "# Calculate cosine similarity\n",
        "print(\"Cosine similarity between vector1 and vector2:\", cosine_similarity(vector1, vector2))\n",
        "print(\"Cosine similarity between vector1 and vector3:\", cosine_similarity(vector1, vector3))\n",
        "print(\"Cosine similarity between vector1 and vector4:\", cosine_similarity(vector1, vector4))\n",
        "\n"
      ],
      "metadata": {
        "id": "oyeuMsxobQ2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_relevant_resources(query: str,\n",
        "                                embeddings: torch.tensor,\n",
        "                                model: SentenceTransformer=embedding_model,\n",
        "                                n_resources_to_return: int=5,\n",
        "                                print_time: bool=True):\n",
        "    \"\"\"\n",
        "    Embeds a query with model and returns top k scores and indices from\n",
        "    \"\"\"\n",
        "\n",
        "    # Embed the query\n",
        "    query_embedding = model.encode(query,\n",
        "                                   convert_to_tensor=True)\n",
        "\n",
        "    # Get dot product scores on embeddings\n",
        "    start_time = timer()\n",
        "    dot_scores = util.dot_score(query_embedding, embeddings)[0]\n",
        "    end_time = timer()\n",
        "\n",
        "    if print_time:\n",
        "        print(\n",
        "            f\"[INFO] Time taken {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")\n",
        "\n",
        "    scores, indices = torch.topk(input=dot_scores,\n",
        "                                 k=n_resources_to_return)\n",
        "\n",
        "    return scores, indices\n",
        "\n",
        "def print_top_results_and_scores(query: str,\n",
        "                                 embeddings: torch.tensor,\n",
        "                                 pages_and_chunks: list[dict]=pages_and_chunks,\n",
        "                                 n_resources_to_return: int=3):\n",
        "    \"\"\"\n",
        "    Takes a query, retrieves most relevant resources and prints them.\n",
        "\n",
        "    Note: Requires pages_and_chunks to be formatted in a specific way\n",
        "    \"\"\"\n",
        "\n",
        "    scores, indices = retrieve_relevant_resources(query=query,\n",
        "                                                  embeddings=embeddings,\n",
        "                                                  n_resources_to_return=n_resources_to_return)\n",
        "\n",
        "    print(f\"Query: {query}\\n\")\n",
        "    print(\"Results:\")\n",
        "    # Loop through zipped together scores and indicies\n",
        "    for score, index in zip(scores, indices):\n",
        "        print(f\"Score: {score:.4f}\")\n",
        "        # Print relevant sentence chunk (since the scores are in descending.\n",
        "        print_wrapped(pages_and_chunks[index][\"sentence_chunk\"])\n",
        "        # Print the page number too so we can reference the textbook further\n",
        "        print(f\"Page number: {pages_and_chunks[index]['page_number']}\")\n",
        "        print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "tdUpZ96Ybkqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "query = \"how to implement a CNN\"\n",
        "\n",
        "# Get just the scores and indices of top related results\n",
        "scores, indices = retrieve_relevant_resources(query=query,\n",
        "                                              embeddings=embeddings)\n",
        "scores, indices"
      ],
      "metadata": {
        "id": "HMz36CNJbuuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_top_results_and_scores(query=query,\n",
        "                             embeddings=embeddings)\n",
        "\n"
      ],
      "metadata": {
        "id": "ZHPlB4kgb1JG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}